{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# This is a magic command. It will display the plotting image directly below the code cell\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat  # for loading MATLAB files\n",
    "\n",
    "dataFilePath = os.getcwd() + 'Data/ex3data1.mat'\n",
    "data = loadmat(dataFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*************************************************************************#\n",
    "    #                 EXTRACTION OF TRAINING DATA: (X,Y)                      #\n",
    "    #                                                                         #\n",
    "    #*************************************************************************#\n",
    "\n",
    "# preparing the training input matrix\n",
    "X  = data['X']                           # (5000,400)-dim array of all training inputs\n",
    "\n",
    "n  = data['X'].shape[1]                  # input layer size = 400\n",
    "m  = data['X'].shape[0]                  # total number of training examples\n",
    "\n",
    "I  = np.ones((m,1))                      # (5000,1)-dim array of ones\n",
    "\n",
    "A1 = np.c_[I, X]                         # (5000,401)-dim array   ### A^(1)\n",
    "A1 = np.matrix(A1)      #numpy matrix    # A^(1) \n",
    "\n",
    "y  = data['y']                           # (5000,1)-dim array of all training outputs                     \n",
    "y  = np.matrix(y)       #numpy matrix    # labels 10,1,...,9 in batches of 500 => (5000,1)-dim                           \n",
    "\n",
    "\n",
    "# preparing the training output matrix\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y = encoder.fit_transform(y)\n",
    "Y = np.matrix(Y)                         # (5000,10)-dim matrix of all training outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*************************************************************************#\n",
    "    #                   PARAMETERS OF THE NEURAL NETWORK                      #\n",
    "    #                                                                         #\n",
    "    #*************************************************************************#\n",
    "\n",
    "L = 25                                   # Hidden Layer Size\n",
    "K = 10                                   # Number of Labels (DO NOT CHANGE THIS unless necessary)\n",
    "reg = 1                                  # Regularization parameter\n",
    "eta = 0.001                              # Learning rate\n",
    "epochs = 100                             # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomlyInitializeWeights(l_in, l_out):\n",
    "    epsilon_init = 0.12\n",
    "    weights = np.random.rand(l_out, 1 + l_in) * 2 * epsilon_init - epsilon_init\n",
    "    weights = np.matrix(weights)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = randomlyInitializeWeights(n,L)\n",
    "theta2 = randomlyInitializeWeights(L,K)\n",
    "\n",
    "#*************************************************************************#\n",
    "# In order to use scipy.optimize, we need to use a 1D array as a variable #\n",
    "# and define a cost function that depends only on that array.             #\n",
    "\n",
    "weights = np.array(np.hstack((theta1.flatten(), theta2.flatten())))[0]\n",
    "#*************************************************************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForwardAll(theta1, theta2):\n",
    "    \n",
    "    #*************************************************************************#\n",
    "    # This will feedforward all of the training data at once.                 #\n",
    "    # For individual predictions, check the rows of A3.                       #\n",
    "    #*************************************************************************#\n",
    "    \n",
    "    Z2 = A1 * theta1.T\n",
    "    G2 = sigmoid(Z2)                   # needed later for calculation of D2\n",
    "    A2 = np.hstack((I, G2))\n",
    "    \n",
    "    Z3 = A2 * theta2.T\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    return G2, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackPropagation(theta1, theta2):\n",
    "    \n",
    "    G2, A2, A3 = FeedForwardAll(theta1, theta2) \n",
    "    \n",
    "    #*********************************Error Terms : Start*************************************** \n",
    "        \n",
    "    theta2_nobias = theta2[:, 1:]  # removes the first column containing the biases\n",
    "    \n",
    "    # (5000,10)-dim matrix of all 10-dim error terms at output:\n",
    "    \n",
    "    D3 = A3 - Y\n",
    "    \n",
    "    # (5000,25)-dim matrix of all 25-dim error terms at the hidden layer:\n",
    "    \n",
    "    D2 = np.multiply(np.multiply(G2,(1-G2)), (D3 * theta2_nobias)) \n",
    "    \n",
    "    #**********************************Error Terms : End**************************************** \n",
    "    \n",
    "    \n",
    "    # Compute the gradients of the regularized cost function\n",
    "    \n",
    "    grad1 = (1/m) * (D2.T * A1) + ((reg/m) * theta1)\n",
    "    grad2 = (1/m) * (D3.T * A2) + ((reg/m) * theta2)\n",
    "    \n",
    "    return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the unregularized cost\n",
    "\n",
    "def cost(A3, Y):\n",
    "    J = -(1/m) * np.trace( Y * np.log(A3.T) + (1-Y) * np.log(1-A3.T) )\n",
    "    return J\n",
    "\n",
    "# definition of the regularized cost\n",
    "\n",
    "def regularizedcost(weights):\n",
    "    \n",
    "    # reconstructing theta1 and theta2\n",
    "    theta1 = np.matrix(weights[:L*(1+n)].reshape(L,1+n))\n",
    "    theta2 = np.matrix(weights[L*(1+n):].reshape(K,1+L))\n",
    "    \n",
    "    A3 = FeedForwardAll(theta1, theta2)[2]\n",
    "    \n",
    "    phi1 = theta1.T * theta1\n",
    "    phi2 = theta2.T * theta2\n",
    "    regularization = (reg/(2 * m)) * (np.trace(phi1) + np.trace(phi2) - phi1[0,0] - phi2[0,0])\n",
    "    \n",
    "    J = cost(A3,Y) + regularization\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcost(weights):\n",
    "    \n",
    "    # reconstructing theta1 and theta2\n",
    "    theta1 = np.matrix(weights[:L*(1+n)].reshape(L,1+n))\n",
    "    theta2 = np.matrix(weights[L*(1+n):].reshape(K,1+L))\n",
    "    \n",
    "    grad1, grad2 = BackPropagation(theta1, theta2)\n",
    "    \n",
    "    # like before, we need to stack them up in a 1D array for np.optimize\n",
    "    lingrad = np.array(np.hstack((grad1.flatten(), grad2.flatten())))[0]\n",
    "    \n",
    "    return lingrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.352896\n",
      "         Iterations: 100\n",
      "         Function evaluations: 227\n",
      "         Gradient evaluations: 227\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg as find_bottom_of\n",
    "\n",
    "    #*************************************************************************#\n",
    "    # We need to optimize the regularized cost function in order to find the  #\n",
    "    # best weights. Since there is no guarantee that there is a global bottom #\n",
    "    # of the cost function, we need to limit the maximum number of iterations #\n",
    "    # to the specified number of epochs.                                      #\n",
    "    #                                                                         #\n",
    "    # If you wish to remove the regularization, just set the regularization   #\n",
    "    # parameter (\"reg\" above) to zero. Lastly, the learning rate decides the  #\n",
    "    # size of the steps taken to slide to the bottom in every iteration.      #\n",
    "    #*************************************************************************#\n",
    "\n",
    "trained_weights = find_bottom_of(f=regularizedcost, x0=weights, fprime=gradcost, epsilon=eta, maxiter=epochs)\n",
    "\n",
    "trained_theta1 = np.matrix(trained_weights[:L*(1+n)].reshape(L,1+n))\n",
    "trained_theta2 = np.matrix(trained_weights[L*(1+n):].reshape(K,1+L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch complete\n",
      "Cost reduced to:  0.3528960103692646\n",
      "Training Accuracy =  98.6 %\n"
     ]
    }
   ],
   "source": [
    "    #*************************************************************************#\n",
    "    #                     Performance on the Training Data                    #\n",
    "    #                                                                         #\n",
    "    #*************************************************************************#\n",
    "    \n",
    "A3 = FeedForwardAll(trained_theta1, trained_theta2)[2]\n",
    "\n",
    "# converting the rows of A3 into lists of mostly zeroes,\n",
    "# and a single value of \"1\" where its most large.\n",
    "\n",
    "for i in range(m): A3[i,np.argmax(A3[i])] = 1\n",
    "pred = (A3 == np.ones((m,K))).astype(float)\n",
    "\n",
    "# checking if the predictions are correct and counting \n",
    "# the total number of correct predictions\n",
    "correct = 0\n",
    "for i in range(m): \n",
    "        if np.array_equal(Y[i],pred[i]): correct += 1\n",
    "\n",
    "print(\"Epoch complete\")\n",
    "print(\"Cost reduced to: \", regularizedcost(trained_weights))\n",
    "print(\"Training Accuracy = \", correct*100/m,\"%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
